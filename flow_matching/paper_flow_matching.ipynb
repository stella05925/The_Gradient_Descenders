{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "view-in-github",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Mamiglia/challenge/blob/master/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Git20q9I3IC1"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1RcpjR928Yt"
   },
   "source": [
    "❕ Add `kaggle.json` API token to files before starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIjKmAeNc5CF"
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "def run(cmd):\n",
    "    \"\"\"Helper to execute shell commands with logging.\"\"\"\n",
    "    print(f\"▶ {cmd}\")\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "# --- Kaggle setup ---\n",
    "if not os.path.exists(os.path.expanduser(\"~/.kaggle/kaggle.json\")):\n",
    "    run(\"mkdir -p ~/.kaggle && cp kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\")\n",
    "\n",
    "# --- Kaggle data download (only if missing) ---\n",
    "if not os.path.exists(\"data/train\"):\n",
    "    run(\"kaggle competitions download -c aml-competition -p data\")\n",
    "    run(\"unzip -qo data/aml-competition.zip -d data\")\n",
    "\n",
    "# --- Clone repositories ---\n",
    "if not os.path.exists(\"challenge\"):\n",
    "    run(\"git clone https://github.com/Mamiglia/challenge.git\")\n",
    "\n",
    "# --- Install dependencies ---\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q openai-clip scikit-learn opencv-python torchdiffeq \\\n",
    "    beautifulsoup4 open_clip_torch scikit-image cython matplotlib accelerate \\\n",
    "    absl-py ml_collections einops wandb ftfy transformers timm tensorboard pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9c9a8587"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F, numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sys, random, logging, math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Extend path to local repositories\n",
    "sys.path.extend([\"challenge/src\"])\n",
    "\n",
    "# Project imports\n",
    "from challenge.src.common import load_data, prepare_train_data, generate_submission\n",
    "from challenge.src.eval import visualize_retrieval\n",
    "\n",
    "# Configure logging\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f5e8e20"
   },
   "outputs": [],
   "source": [
    "# Configuration dictionary\n",
    "CFG = {\n",
    "    \"MODEL_PATH\": \"models/crossflow2.pth\",\n",
    "    \"SEED\": 42,\n",
    "    \"DEVICE\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "    # --- Data Dimensions ---\n",
    "    \"TEXT_DIM\": 1024,\n",
    "    \"LATENT_DIM\": 1536,\n",
    "\n",
    "    # --- Model Architecture ---\n",
    "    \"TIME_EMB_DIM\": 256,\n",
    "    \"N_LAYERS_VE\": 3,   # Variational Encoder layers\n",
    "    \"N_HEADS_VE\": 8,\n",
    "    \"N_LAYERS_FLOW\": 6, # Flow Matching layers\n",
    "    \"N_HEADS_FLOW\": 8,\n",
    "\n",
    "    # --- Training ---\n",
    "    \"EPOCHS\": 2,\n",
    "    \"BATCH_SIZE\": 16,\n",
    "    \"LR\": 1e-4,\n",
    "    \"WEIGHT_DECAY\": 0.03,\n",
    "    \"KL_WEIGHT\": 1e-4,\n",
    "    \"UNCOND_RATE\": 0.1,\n",
    "\n",
    "    # --- Inference ---\n",
    "    \"INFERENCE_STEPS\": 10,\n",
    "    \"GUIDANCE_SCALE\": 1.7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9SGeyg0Bi0w"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"Ensure deterministic reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(CFG[\"SEED\"])\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-UnvZXPBjcc"
   },
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykY3JTmt3nWU"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Utility Functions\n",
    "# ============================================================\n",
    "\n",
    "def reparametrize(mu, logvar):\n",
    "    std = (0.5 * logvar).exp()\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "\n",
    "def compute_z_t_and_vhat(z0, z1, t, sigma_min=1e-5):\n",
    "    \"\"\"\n",
    "    Flow interpolation & target velocity from CrossFlow paper (Eq. 1)\n",
    "    \"\"\"\n",
    "    B, _, D = z0.shape\n",
    "    t = t.view(B, 1, 1)  # ensure proper broadcast shape\n",
    "    z_t = t * z1 + (1.0 - (1.0 - sigma_min) * t) * z0\n",
    "    v_hat = z1 - (1.0 - sigma_min) * z0\n",
    "    return z_t, v_hat\n",
    "\n",
    "def sinusoidal_time_embedding(t, dim):\n",
    "    \"\"\"\n",
    "    t: [B,1] with values in [0,1]\n",
    "    returns: [B, dim] time embedding (sin/cos)\n",
    "    \"\"\"\n",
    "    # follow common diffusion embeddings: scale by 2pi\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(-math.log(1e4) * torch.arange(0, half, dtype=torch.float32, device=t.device) / (half - 1))\n",
    "    args = t * 2.0 * math.pi * freqs.view(1, -1)  # [B, half]\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "    if dim % 2 == 1:\n",
    "        # pad one zero if odd\n",
    "        emb = F.pad(emb, (0, 1))\n",
    "    return emb  # [B, dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdGOGO-cNQyq"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Components: Variational Encoder & Flow Transformer\n",
    "# ============================================================\n",
    "\n",
    "class VariationalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight Transformer-based Variational Encoder.\n",
    "    Input:  text embedding  [B, N, text_dim]\n",
    "    Output: mu, logvar each [B, latent_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, text_dim, latent_dim, n_layers=3, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.proj_in = nn.Linear(text_dim, latent_dim)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=latent_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=latent_dim * 4,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.to_mu = nn.Linear(latent_dim, latent_dim)\n",
    "        self.to_logvar = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):  # x:[B,N,text_dim]\n",
    "        h = self.proj_in(x)        # [B,N,latent_dim]\n",
    "        h = h.permute(1, 0, 2)     # [N,B,E]\n",
    "        h = self.encoder(h)\n",
    "        h = h.permute(1, 0, 2)     # [B,N,E]\n",
    "        pooled = h.mean(dim=1)     # [B,E]\n",
    "        mu, logvar = self.to_mu(pooled), self.to_logvar(pooled)\n",
    "        return mu, logvar\n",
    "\n",
    "class FlowTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Predict velocity per sample. Supports per-sample indicator mask (boolean tensor [B]),\n",
    "    and richer sinusoidal time embedding injection.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, n_layers=6, n_heads=8, time_emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_proj = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=latent_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=latent_dim * 4,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "        # CFG tokens: two learnable tokens (conditional & unconditional)\n",
    "        self.g_c = nn.Parameter(torch.randn(1, latent_dim))\n",
    "        self.g_uc = nn.Parameter(torch.randn(1, latent_dim))\n",
    "\n",
    "        # final output projection to velocity (per-token)\n",
    "        self.out_proj = nn.Linear(latent_dim, latent_dim)\n",
    "        self.time_emb_dim = time_emb_dim\n",
    "\n",
    "    def forward(self, zt, t, indicator_mask):\n",
    "        B, S, D = zt.shape\n",
    "        h = self.input_proj(zt)  # [B,S,D]\n",
    "\n",
    "        # Append per-sample indicator token\n",
    "        token_cond = self.g_c.expand(B, -1, -1)\n",
    "        token_uncond = self.g_uc.expand(B, -1, -1)\n",
    "        token = torch.where(indicator_mask.view(B, 1, 1), token_cond, token_uncond)  # [B,1,D]\n",
    "        h = torch.cat([h, token], dim=1)  # [B, S+1, D]\n",
    "\n",
    "        # add time embedding (broadcast to tokens)\n",
    "        te = sinusoidal_time_embedding(t, self.time_emb_dim)  # [B, time_emb_dim]\n",
    "        te = self.time_mlp(te)  # [B, D]\n",
    "        h = h + te.unsqueeze(1)  # broadcast to [B, S+1, D]\n",
    "\n",
    "        h = self.transformer(h)  # [B, S+1, D]\n",
    "\n",
    "        # drop the token output and map to velocity for tokens only\n",
    "        h_tokens = h[:, :S, :]  # [B, S, D]\n",
    "        v = self.out_proj(h_tokens)  # [B, S, D]\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asMGKzG633sA"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Composite CrossFlow model\n",
    "# ============================================================\n",
    "\n",
    "class CrossFlowModel(nn.Module):\n",
    "    def __init__(self, text_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.ve = VariationalEncoder(text_dim, latent_dim,\n",
    "                                     n_layers=CFG[\"N_LAYERS_VE\"], n_heads=CFG[\"N_HEADS_VE\"])\n",
    "        self.flow = FlowTransformer(latent_dim,\n",
    "                                    n_layers=CFG[\"N_LAYERS_FLOW\"], n_heads=CFG[\"N_HEADS_FLOW\"],\n",
    "                                    time_emb_dim=CFG[\"TIME_EMB_DIM\"])\n",
    "\n",
    "    def encoding_mu_logvar(self, text_emb):\n",
    "        mu, logvar = self.ve(text_emb)\n",
    "        return mu, logvar\n",
    "\n",
    "    def sample_z0(self, mu, logvar):\n",
    "        return reparametrize(mu, logvar)\n",
    "\n",
    "    def predict_velocity(self, zt, t, indicator_mask):\n",
    "        return self.flow(zt, t, indicator_mask)\n",
    "\n",
    "    def forward_flow_training(self, text_emb, target_latent, t, indicator_mask):\n",
    "        \"\"\"\n",
    "        Training forward: returns v_pred (per-sample), v_hat, mu, logvar\n",
    "        \"\"\"\n",
    "        mu, logvar = self.ve(text_emb)\n",
    "        z0 = reparametrize(mu, logvar)  # [B, latent_dim] flattened\n",
    "        z1 = target_latent\n",
    "\n",
    "        # reshape to token shape: [B, seq_len, D] where seq_len=1\n",
    "        z0_toks = z0.unsqueeze(1)\n",
    "        z1_toks = z1.unsqueeze(1)\n",
    "\n",
    "        # compute z_t and v_hat\n",
    "        z_t, v_hat = compute_z_t_and_vhat(z0_toks, z1_toks, t)\n",
    "\n",
    "        z_t = z_t.view(z_t.size(0), 1, -1)\n",
    "        v_hat = v_hat.view(v_hat.size(0), 1, -1)\n",
    "\n",
    "        # predict\n",
    "        v_pred = self.predict_velocity(z_t, t, indicator_mask)\n",
    "        return v_pred, v_hat, mu, logvar\n",
    "\n",
    "    def predict_z1_from_z0(self, z0, n_steps=10, guidance_scale=1.0, indicator_mask=None):\n",
    "        \"\"\"\n",
    "        Integrate the flow from z0 -> z1_pred using Euler steps.\n",
    "        If guidance_scale != 1.0, we compute both cond/uncond predictions inside integrator.\n",
    "        \"\"\"\n",
    "        B = z0.shape[0]\n",
    "        z = z0.unsqueeze(1)  # [B,1,D]\n",
    "        device = z.device\n",
    "        for i in range(n_steps):\n",
    "            t = torch.full((B, 1), float(i + 1) / n_steps, device=device)\n",
    "            if guidance_scale == 1.0:\n",
    "                # single pass cond (assume conditioned)\n",
    "                indicator_mask = torch.ones(B, dtype=torch.bool, device=device)\n",
    "                v = self.predict_velocity(z, t, indicator_mask)\n",
    "            else:\n",
    "                # compute both conditional and unconditional predictions in batch\n",
    "                cond_mask = torch.ones(B, dtype=torch.bool, device=device)\n",
    "                uncond_mask = torch.zeros(B, dtype=torch.bool, device=device)\n",
    "                v_cond = self.predict_velocity(z, t, cond_mask)\n",
    "                v_uncond = self.predict_velocity(z, t, uncond_mask)\n",
    "                v = guidance_scale * v_cond + (1.0 - guidance_scale) * v_uncond\n",
    "\n",
    "            z = z + v / float(n_steps)  # Euler step\n",
    "        z1_pred = z.squeeze(1)  # [B,D]\n",
    "        return z1_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqdFc0Tb6cq9"
   },
   "source": [
    "# Visualization Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSfYQrtr2i6H"
   },
   "outputs": [],
   "source": [
    "# Color palette\n",
    "source_color = [0/255, 114/255, 178/255]\n",
    "target_color = [213/255, 94/255, 0/255]\n",
    "plt.style.use(\"default\")\n",
    "\n",
    "def plot_distributions(dist1, dist2, title1=\"Source\", title2=\"Target\"):\n",
    "    \"\"\"Project two embedding sets into 2D with PCA and plot side-by-side.\"\"\"\n",
    "    X1_2d = PCA(n_components=2).fit_transform(dist1.cpu())\n",
    "    X2_2d = PCA(n_components=2).fit_transform(dist2.cpu())\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.scatter(X1_2d[:, 0], X1_2d[:, 1], color=source_color, alpha=0.6, s=8)\n",
    "    ax2.scatter(X2_2d[:, 0], X2_2d[:, 1], color=target_color, alpha=0.6, s=8)\n",
    "    ax1.set_title(title1); ax2.set_title(title2)\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.set_aspect(\"equal\"); ax.axis(\"off\")\n",
    "    plt.tight_layout(); plt.show(); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MMwXFp96q2u"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75dc85c5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare training and validation sets\n",
    "\"\"\"\n",
    "train_data = load_data(\"data/train/train/train.npz\")\n",
    "X, y, label = prepare_train_data(train_data)\n",
    "\n",
    "# Split train/val\n",
    "n_train = int(0.9 * len(X))\n",
    "TRAIN_SPLIT = torch.zeros(len(X), dtype=torch.bool)\n",
    "TRAIN_SPLIT[:n_train] = 1\n",
    "X_train, X_val = X[TRAIN_SPLIT], X[~TRAIN_SPLIT]\n",
    "y_train, y_val = y[TRAIN_SPLIT], y[~TRAIN_SPLIT]\n",
    "\n",
    "# Data Loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG[\"BATCH_SIZE\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG[\"BATCH_SIZE\"])\n",
    "\n",
    "# Pre-compute masks for validation/viz\n",
    "img_VAL_SPLIT = label[~TRAIN_SPLIT].sum(dim=0) > 0\n",
    "val_img_file = train_data['images/names'][img_VAL_SPLIT]\n",
    "val_img_embd = torch.from_numpy(train_data['images/embeddings'][img_VAL_SPLIT])\n",
    "val_label = np.nonzero(train_data['captions/label'][~TRAIN_SPLIT][:,img_VAL_SPLIT])[1]\n",
    "val_caption_text = train_data['captions/text'][~TRAIN_SPLIT]\n",
    "\n",
    "print(f\"Train data: {len(X_train)} samples. Val data: {len(X_val)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIayUzZG6ajl"
   },
   "outputs": [],
   "source": [
    "# Plot the data distributions\n",
    "print(\"Plotting data distributions:\")\n",
    "plot_distributions(X_train[:2000], y_train[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSMk-1jJ9zLr"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4Dq8QNM5r4S"
   },
   "outputs": [],
   "source": [
    "def train_crossflow_model(model, train_loader, val_loader, n_epochs, lr):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=CFG[\"WEIGHT_DECAY\"])\n",
    "    model.to(CFG[\"DEVICE\"])\n",
    "    \n",
    "    best_val = float(\"inf\")\n",
    "    Path(CFG[\"MODEL_PATH\"]).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=False)\n",
    "        for Xb, Yb in pbar:\n",
    "            Xb, Yb = Xb.to(CFG[\"DEVICE\"]), Yb.to(CFG[\"DEVICE\"])\n",
    "            t = torch.rand(Xb.size(0), 1, device=CFG[\"DEVICE\"])\n",
    "\n",
    "            # create boolean mask: True = conditioned, False = unconditioned (dropped)\n",
    "            B = Xb.size(0)\n",
    "            indicator_mask = torch.rand(B, device=CFG[\"DEVICE\"]) > CFG[\"UNCOND_RATE\"]\n",
    "            \n",
    "            v_pred, v_hat, mu, logvar = model.forward_flow_training(Xb.unsqueeze(1), Yb, t, indicator_mask)\n",
    "\n",
    "            # Losses\n",
    "            L_FM = F.mse_loss(v_pred, v_hat)\n",
    "            L_KL = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            # Simple cosine contrastive term\n",
    "            L_enc = 1 - F.cosine_similarity(mu, Yb).mean()\n",
    "\n",
    "            loss = L_FM + L_enc + CFG[\"KL_WEIGHT\"] * L_KL\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        retrieval_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for Xb, Yb in val_loader:\n",
    "                Xb, Yb = Xb.to(CFG[\"DEVICE\"]), Yb.to(CFG[\"DEVICE\"])\n",
    "                mu, logvar = model.encoding_mu_logvar(Xb.unsqueeze(1))\n",
    "                z0 = mu  # use mean for deterministic inference\n",
    "                z1_pred = model.predict_z1_from_z0(z0, n_steps=10, guidance_scale=1.0)\n",
    "                # retrieval metric: cosine\n",
    "                sim = F.cosine_similarity(z1_pred, Yb, dim=-1)  # [B]\n",
    "                batch_loss = 1.0 - sim.mean()\n",
    "                retrieval_loss += batch_loss.item()\n",
    "        retrieval_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"> Epoch {epoch+1}: Train Loss {train_loss:.6f} | Val Loss {retrieval_loss:.6f}\")\n",
    "\n",
    "        if retrieval_loss < best_val:\n",
    "            best_val = retrieval_loss\n",
    "            torch.save(model.state_dict(), CFG[\"MODEL_PATH\"])\n",
    "            print(f\"  ✓ Saved best model (val={retrieval_loss:.6f})\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdqCbx694RQ1"
   },
   "outputs": [],
   "source": [
    "# Initialize and Train\n",
    "print(\"Initializing CrossFlow Model...\")\n",
    "model = CrossFlowModel(CFG[\"TEXT_DIM\"], CFG[\"LATENT_DIM\"])\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "model = train_crossflow_model(model, train_loader, val_loader, CFG[\"EPOCHS\"], CFG[\"LR\"])\n",
    "\n",
    "# Load best model for inference\n",
    "model.load_state_dict(torch.load(CFG[\"MODEL_PATH\"]))\n",
    "model.to(CFG[\"DEVICE\"])\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a533e6f"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hoWxcQdm3Uw"
   },
   "outputs": [],
   "source": [
    "def predict_embeddings(model, text_embs, n_steps=10, guidance_scale=1.0, deterministic=True, device=CFG[\"DEVICE\"]):\n",
    "    \"\"\"\n",
    "    Helper to generate image embeddings from text embeddings using the trained CrossFlow model.\n",
    "    text_embs: torch.Tensor shape [B, text_dim] OR [B, 1, text_dim]\n",
    "    Returns: pred_embds shape [B, latent_dim] (cpu tensor)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # ensure shape is [B, 1, text_dim] for the VE\n",
    "        if text_embs.ndim == 1:\n",
    "            text_embs = text_embs.unsqueeze(0)        # [1, text_dim]\n",
    "        if text_embs.ndim == 2:\n",
    "            text_in = text_embs.unsqueeze(1).to(device)  # [B,1,text_dim]\n",
    "        else:\n",
    "            text_in = text_embs.to(device)  # already [B,1,text_dim] or [B,N,text_dim]\n",
    "\n",
    "        mu, logvar = model.encoding_mu_logvar(text_in)   # [B, latent_dim]\n",
    "        if deterministic:\n",
    "            z0 = mu\n",
    "        else:\n",
    "            z0 = model.sample_z0(mu, logvar)\n",
    "\n",
    "        z1_pred = model.predict_z1_from_z0(z0, n_steps=n_steps, guidance_scale=guidance_scale)\n",
    "        return z1_pred.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d060a9e8"
   },
   "outputs": [],
   "source": [
    "# Sample and visualize retrieval results\n",
    "print(\"Visualizing retrieval examples...\")\n",
    "model.eval()\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(0, 100)\n",
    "    caption_embd = val_dataset[idx][0] # Get text embedding\n",
    "    caption_text = val_caption_text[idx]\n",
    "    gt_index = val_label[idx]\n",
    "\n",
    "    # Predict\n",
    "    pred_embds = predict_embeddings(\n",
    "        model, \n",
    "        caption_embd.to(CFG[\"DEVICE\"]), \n",
    "        n_steps=CFG[\"INFERENCE_STEPS\"], \n",
    "        guidance_scale=CFG[\"GUIDANCE_SCALE\"]\n",
    "    ).squeeze(0)\n",
    "\n",
    "    visualize_retrieval(\n",
    "            pred_embds,\n",
    "            gt_index,\n",
    "            val_img_file,\n",
    "            caption_text, val_img_embd, k=5,\n",
    "            dataset_path=\"data/train/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09d07798"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "156b49a0"
   },
   "outputs": [],
   "source": [
    "print(\"Generating submission file...\")\n",
    "test_data = load_data(\"data/test/test/test.clean.npz\")\n",
    "test_embds = torch.from_numpy(test_data['captions/embeddings']).float()\n",
    "\n",
    "# Generate predicted embeddings in batches\n",
    "all_preds = []\n",
    "BS = 64\n",
    "for i in range(0, len(test_embds), BS):\n",
    "    batch = test_embds[i:i+BS]\n",
    "    preds = predict_embeddings(\n",
    "        model, \n",
    "        batch, \n",
    "        n_steps=CFG[\"INFERENCE_STEPS\"], \n",
    "        guidance_scale=CFG[\"GUIDANCE_SCALE\"], \n",
    "        device=CFG[\"DEVICE\"]\n",
    "    )\n",
    "    all_preds.append(preds)\n",
    "\n",
    "pred_embds = torch.cat(all_preds, dim=0)\n",
    "print(f\"Predicted test embeddings shape: {pred_embds.shape}\")\n",
    "\n",
    "generate_submission(test_data['captions/ids'], pred_embds, 'submission.csv')\n",
    "print(\"submission.csv generated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}