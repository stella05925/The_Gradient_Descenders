{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f130dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.common import TRAIN_DATA, TEST_DATA, DEVICE\n",
    "from src.common import load_data, prepare_train_data, generate_submission\n",
    "from CrossFlow.diffusion.flow_matching import ClipLoss, SigLipLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76794389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_procrustes_with_padding(X, Y, allow_reflection=False):\n",
    "    d1, d2 = X.shape[1], Y.shape[1]\n",
    "    d_max = max(d1, d2)\n",
    "\n",
    "    if d1 < d_max:\n",
    "        X = np.pad(X, ((0, 0), (0, d_max - d1)), mode='constant')\n",
    "    if d2 < d_max:\n",
    "        Y = np.pad(Y, ((0, 0), (0, d_max - d2)), mode='constant')\n",
    "    \n",
    "    H = X.T @ Y\n",
    "    U, S, Vt = np.linalg.svd(H, full_matrices=False)\n",
    "    R = U @ Vt\n",
    "    \n",
    "    if not allow_reflection and np.linalg.det(R) < 0:\n",
    "        U[:, -1] *= -1\n",
    "        R = U @ Vt\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c514853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(X, method='standard', stats=None):\n",
    "    if method == 'none':\n",
    "        return X, {'method': 'none', 'dim': X.shape[1]}\n",
    "    \n",
    "    elif method == 'l2':\n",
    "\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "        norms = np.where(norms == 0, 1, norms)\n",
    "        X_norm = X / norms\n",
    "        \n",
    "        if stats is None:\n",
    "            stats = {\n",
    "                'method': 'l2',\n",
    "                'dim': X.shape[1]\n",
    "            }\n",
    "        \n",
    "        return X_norm, stats\n",
    "    \n",
    "    elif method == 'standard':\n",
    "        if stats is None:\n",
    "            mean = X.mean(axis=0)\n",
    "            std = X.std(axis=0)\n",
    "            std = np.where(std == 0, 1, std)\n",
    "            \n",
    "            stats = {\n",
    "                'method': 'standard',\n",
    "                'mean': mean,\n",
    "                'std': std,\n",
    "                'dim': X.shape[1]\n",
    "            }\n",
    "        else:\n",
    "            mean = stats['mean']\n",
    "            std = stats['std']\n",
    "        \n",
    "        X_norm = (X - mean) / std\n",
    "        \n",
    "        return X_norm, stats\n",
    "    \n",
    "def denormalize_embeddings(X_norm, method='standard', stats=None):\n",
    "    if method == 'none':\n",
    "        return X_norm\n",
    "    \n",
    "    elif method == 'l2':\n",
    "        return X_norm\n",
    "    \n",
    "    elif method == 'standard':\n",
    "        if stats is None:\n",
    "            raise ValueError(\"Need stats for denormalization\")\n",
    "        \n",
    "        mean = stats['mean']\n",
    "        std = stats['std']\n",
    "        \n",
    "        if X_norm.shape[1] > len(mean):\n",
    "            X_norm = X_norm[:, :len(mean)]\n",
    "        \n",
    "        X = X_norm * std + mean\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eda6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_anchors_diverse(train_data, n_anchors, method='uniform'):\n",
    "    caption_embeddings = train_data['captions/embeddings']\n",
    "    label_matrix = train_data['captions/label']\n",
    "    \n",
    "    n_captions = len(caption_embeddings)\n",
    "    \n",
    "    gt_indices = np.argmax(label_matrix, axis=1)\n",
    "    \n",
    "    if method == 'uniform':\n",
    "        caption_indices = np.linspace(0, n_captions - 1, n_anchors, dtype=int)\n",
    "        \n",
    "    elif method == 'random':\n",
    "        caption_indices = np.random.choice(n_captions, n_anchors, replace=False)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    image_indices = gt_indices[caption_indices]\n",
    "    \n",
    "    print(f\"   Selected {len(caption_indices)} anchor pairs\")\n",
    "    print(f\"   Caption indices range: {caption_indices.min()} - {caption_indices.max()}\")\n",
    "    print(f\"   Image indices range: {image_indices.min()} - {image_indices.max()}\")\n",
    "    \n",
    "    return caption_indices, image_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "181a641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcrustesTranslator:\n",
    "    \"\"\"\n",
    "    Zero-shot translator using Procrustes analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, normalization='standard', method='ortho'):\n",
    "        self.normalization = normalization\n",
    "        self.method = method\n",
    "        self.R = None\n",
    "        self.scale = None\n",
    "        self.bias = None\n",
    "        self.source_stats = None\n",
    "        self.target_stats = None\n",
    "        self.orthogonality_error = None\n",
    "    \n",
    "    def fit(self, X_source, Y_target):\n",
    "        if torch.is_tensor(X_source):\n",
    "            X_source = X_source.cpu().numpy()\n",
    "        if torch.is_tensor(Y_target):\n",
    "            Y_target = Y_target.cpu().numpy()\n",
    "        \n",
    "        print(f\"  Input dimensions: {X_source.shape[1]} -> {Y_target.shape[1]}\")\n",
    "        print(f\"  Method: {self.method}\")\n",
    "\n",
    "        X_norm, self.source_stats = normalize_embeddings(\n",
    "            X_source, self.normalization\n",
    "        )\n",
    "        Y_norm, self.target_stats = normalize_embeddings(\n",
    "            Y_target, self.normalization\n",
    "        )\n",
    "\n",
    "        if self.method == 'ortho':\n",
    "            self.R, self.orthogonality_error = compute_procrustes_with_padding(\n",
    "                X_norm, Y_norm, allow_reflection=False\n",
    "            )\n",
    "            self.scale = 1.0\n",
    "            self.bias = None\n",
    "            \n",
    "        elif self.method == 'lortho':\n",
    "            self._fit_linear(X_norm, Y_norm)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {self.method}\")\n",
    "        \n",
    "        print(f\"  Transformation matrix shape: {self.R.shape}\")\n",
    "        if self.orthogonality_error is not None:\n",
    "            print(f\"  Orthogonality check: {self.orthogonality_error:.6e}\")\n",
    "    \n",
    "    def _fit_linear(self, X_norm, Y_norm):\n",
    "        \"\"\"Fit linear transformation (least squares).\"\"\"\n",
    "        d_max = max(X_norm.shape[1], Y_norm.shape[1])\n",
    "\n",
    "        X_padded = np.pad(X_norm, ((0, 0), (0, d_max - X_norm.shape[1])))\n",
    "        Y_padded = np.pad(Y_norm, ((0, 0), (0, d_max - Y_norm.shape[1])))\n",
    "\n",
    "        T = np.linalg.lstsq(X_padded, Y_padded, rcond=None)[0]\n",
    "\n",
    "        U, S, Vt = np.linalg.svd(T, full_matrices=False)\n",
    "        self.R = U @ Vt\n",
    "        self.scale = np.mean(S)\n",
    "        \n",
    "        self.orthogonality_error = np.linalg.norm(self.R.T @ self.R - np.eye(self.R.shape[0]))\n",
    "    \n",
    "    def transform(self, X_source):\n",
    "\n",
    "        was_tensor = torch.is_tensor(X_source)\n",
    "        if was_tensor:\n",
    "            device = X_source.device\n",
    "            X_source = X_source.cpu().numpy()\n",
    "\n",
    "        X_norm, _ = normalize_embeddings(\n",
    "            X_source, self.normalization, self.source_stats\n",
    "        )\n",
    "\n",
    "        d_in = X_norm.shape[1]\n",
    "        d_R = self.R.shape[0]\n",
    "        \n",
    "        if d_in < d_R:\n",
    "            X_norm = np.pad(X_norm, ((0, 0), (0, d_R - d_in)), mode='constant')\n",
    "\n",
    "        if self.method == 'ortho':\n",
    "            Y_norm = X_norm @ self.R\n",
    "        elif self.method == 'lortho':\n",
    "            Y_norm = (X_norm @ self.R) * self.scale\n",
    "        elif self.method == 'affine':\n",
    "            Y_norm = (X_norm @ self.R) * self.scale + self.bias\n",
    "        \n",
    "        if self.normalization == 'standard':\n",
    "            d_target = len(self.target_stats['mean'])\n",
    "        elif self.normalization == 'l2':\n",
    "            d_target = self.target_stats['dim']\n",
    "        else:\n",
    "            d_target = d_R\n",
    "        \n",
    "        if Y_norm.shape[1] > d_target:\n",
    "            Y_norm = Y_norm[:, :d_target]\n",
    "        \n",
    "        Y = denormalize_embeddings(\n",
    "            Y_norm, self.normalization, self.target_stats\n",
    "        )\n",
    "\n",
    "        if was_tensor:\n",
    "            Y = torch.from_numpy(Y).float().to(device)\n",
    "        \n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86bcb83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class RefinedProcrustesTranslator(nn.Module):\n",
    "    def __init__(self, d_in=1024, d_out=1536, procrustes_R=None, \n",
    "                 hidden_dim=None, refinement_type='residual'):\n",
    "        super().__init__()\n",
    "        \n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = d_out\n",
    "        \n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.refinement_type = refinement_type\n",
    "        \n",
    "        if procrustes_R is not None:\n",
    "            R_sub = procrustes_R[:d_out, :d_in]\n",
    "            \n",
    "            self.linear = nn.Linear(d_in, d_out, bias=False)\n",
    "            with torch.no_grad():\n",
    "                self.linear.weight.data = torch.from_numpy(R_sub).float()\n",
    "        \n",
    "        # Refinement architecture\n",
    "        if refinement_type == 'residual':\n",
    "            self.refinement = nn.Sequential(\n",
    "                nn.Linear(d_out, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.LayerNorm(hidden_dim // 2),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(hidden_dim // 2, d_out)\n",
    "            )\n",
    "            nn.init.zeros_(self.refinement[-1].weight)\n",
    "            nn.init.zeros_(self.refinement[-1].bias)\n",
    "            \n",
    "            self.residual_weight = nn.Parameter(torch.tensor(0.1))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown refinement_type: {refinement_type}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x_proj = self.linear(x)\n",
    "\n",
    "        if self.refinement_type == 'residual':\n",
    "            residual = self.refinement(x_proj)\n",
    "            alpha = torch.sigmoid(self.residual_weight) * 0.3\n",
    "            output = x_proj + alpha * residual\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30eaf259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_refined_model_improved(\n",
    "    procrustes_translator,\n",
    "    train_data,\n",
    "    val_data=None,\n",
    "    epochs=20,\n",
    "    lr=1e-4,\n",
    "    batch_size=256,\n",
    "    hidden_dim=None,\n",
    "    patience=10,\n",
    "    device='cuda'\n",
    "):\n",
    "    text_embeddings = torch.from_numpy(train_data['captions/embeddings']).float()\n",
    "    image_embeddings = torch.from_numpy(train_data['images/embeddings']).float().to(device)\n",
    "    label_matrix = torch.from_numpy(train_data['captions/label']).bool()\n",
    "    gt_indices = torch.argmax(label_matrix.long(), dim=1)\n",
    "    \n",
    "    print(f\"Training data: {len(text_embeddings)} captions, {len(image_embeddings)} images\")\n",
    "    \n",
    "    if val_data is not None:\n",
    "        val_text = torch.from_numpy(val_data['captions/embeddings']).float()\n",
    "        val_image = torch.from_numpy(val_data['images/embeddings']).float().to(device)\n",
    "        val_label_matrix = torch.from_numpy(val_data['captions/label']).bool()\n",
    "        val_gt_indices = torch.argmax(val_label_matrix.long(), dim=1)\n",
    "        print(f\"Validation data: {len(val_text)} captions\")\n",
    "    \n",
    "    model = RefinedProcrustesTranslator(\n",
    "        d_in=text_embeddings.shape[1],\n",
    "        d_out=image_embeddings.shape[1],\n",
    "        procrustes_R=procrustes_translator.R,\n",
    "        hidden_dim=hidden_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    dataset = TensorDataset(text_embeddings, gt_indices)\n",
    "    loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=2, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    siglip_loss_fn = SigLipLoss().to(device)\n",
    "    \n",
    "    logit_scale = torch.tensor(np.exp(np.log(1.0 / 0.07)), device=device, dtype=torch.float32)\n",
    "    logit_bias = torch.tensor(0.0, device=device, dtype=torch.float32)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "\n",
    "    warmup_epochs = max(1, epochs // 10)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=epochs - warmup_epochs, eta_min=lr/10\n",
    "    )\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [] if val_data else None,\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch_text, batch_gt_idx in pbar:\n",
    "            batch_text = batch_text.to(device)\n",
    "            batch_gt_idx = batch_gt_idx.to(device)\n",
    "\n",
    "            pred_embeddings = model(batch_text)\n",
    "            target_embeddings = image_embeddings[batch_gt_idx]\n",
    "\n",
    "            loss = siglip_loss_fn(target_embeddings, pred_embeddings, logit_scale, logit_bias)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        if epoch >= warmup_epochs:\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if val_data is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = model(val_text.to(device))\n",
    "                val_target = val_image[val_gt_indices]\n",
    "                val_loss = siglip_loss_fn(val_target, val_pred, logit_scale, logit_bias).item()\n",
    "                \n",
    "                history['val_loss'].append(val_loss)\n",
    "                print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "                current_loss = val_loss\n",
    "        else:\n",
    "            current_loss = avg_loss\n",
    "        \n",
    "        print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"  New best model! (Loss: {best_loss:.4f})\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{patience})\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\n  Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f\"Training Complete! Best Loss: {best_loss:.4f}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcc556b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(model, query_embeddings, gallery_embeddings, gt_indices, device, k=10):\n",
    "    # Apply model if provided\n",
    "    if model is not None:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if not torch.is_tensor(query_embeddings):\n",
    "                query_embeddings = torch.from_numpy(query_embeddings).float()\n",
    "            query_embeddings = model(query_embeddings.to(device))\n",
    "    \n",
    "    query_embeddings = query_embeddings.to(device)\n",
    "    gallery_embeddings = gallery_embeddings.to(device)\n",
    "    gt_indices = gt_indices.to(device)\n",
    "    \n",
    "    n_queries = query_embeddings.shape[0]\n",
    "    \n",
    "    # Normalize for cosine similarity\n",
    "    query_norm = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)\n",
    "    gallery_norm = torch.nn.functional.normalize(gallery_embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Compute similarities in batches\n",
    "    batch_size = 1000\n",
    "    all_top_k_indices = []\n",
    "    all_distances = []\n",
    "    \n",
    "    for i in range(0, n_queries, batch_size):\n",
    "        batch_queries = query_norm[i:i+batch_size]\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarities = batch_queries @ gallery_norm.T  # [batch_size, N_gallery]\n",
    "        \n",
    "        # Get top-k predictions\n",
    "        top_k_indices = torch.topk(similarities, k=min(k, similarities.shape[1]), dim=1).indices\n",
    "        all_top_k_indices.append(top_k_indices)\n",
    "        \n",
    "        # Compute L2 distance to ground truth\n",
    "        batch_gt = gt_indices[i:i+batch_size]\n",
    "        gt_embeddings = gallery_embeddings[batch_gt]\n",
    "        distances = torch.norm(query_embeddings[i:i+batch_size] - gt_embeddings, dim=1)\n",
    "        all_distances.append(distances)\n",
    "    \n",
    "    all_top_k_indices = torch.cat(all_top_k_indices, dim=0)\n",
    "    all_distances = torch.cat(all_distances, dim=0)\n",
    "    \n",
    "    # Calculate Recall@k\n",
    "    gt_indices_expanded = gt_indices.unsqueeze(1).expand(-1, all_top_k_indices.shape[1])\n",
    "    correct = (all_top_k_indices == gt_indices_expanded).any(dim=1)\n",
    "    recall_at_k = correct.float().mean().item()\n",
    "    \n",
    "    # Calculate MRR (Mean Reciprocal Rank)\n",
    "    reciprocal_ranks = []\n",
    "    for i in range(n_queries):\n",
    "        pred_indices = all_top_k_indices[i]\n",
    "        gt_idx = gt_indices[i]\n",
    "        \n",
    "        if gt_idx in pred_indices:\n",
    "            rank = (pred_indices == gt_idx).nonzero(as_tuple=True)[0].item() + 1\n",
    "            reciprocal_ranks.append(1.0 / rank)\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "    \n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    mean_l2_dist = all_distances.mean().item()\n",
    "    \n",
    "    return recall_at_k, mrr, mean_l2_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bc71d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Loading training data\n",
      "(125000,)\n",
      "Train data: 125000 captions, 125000 images\n",
      "   Training captions: 120000\n",
      "   Validation captions: 5000\n",
      "   ✓ Saved validation split to data/splits/validation_split.npz\n",
      "\n",
      "2. Selecting 120000 anchor pairs (uniform)...\n",
      "   Selected 120000 anchor pairs\n",
      "   Caption indices range: 0 - 119999\n",
      "   Image indices range: 0 - 24999\n",
      "(120000,)\n",
      "Train data: 120000 captions, 120000 images\n",
      "   Anchor text embeddings: torch.Size([120000, 1024])\n",
      "   Anchor image embeddings: torch.Size([120000, 1536])\n",
      "\n",
      "3. Training Procrustes models\n",
      "   Method: lortho\n",
      "\n",
      "Testing normalization: l2\n",
      "  Input dimensions: 1024 -> 1536\n",
      "  Method: lortho\n",
      "  Transformation matrix shape: (1536, 1536)\n",
      "  Orthogonality check: 1.295021e-05\n",
      "     Validation Results:\n",
      "       Recall@1:  0.1190\n",
      "       Recall@5:  0.2860\n",
      "       Recall@10: 0.3778\n",
      "       MRR:       0.1190\n",
      "       L2 Distance: 25.9078\n",
      "\n",
      "   → Best Procrustes: l2 with MRR = 0.1190\n",
      "Procrustes normalization method: l2\n",
      "Procrustes method: lortho\n"
     ]
    }
   ],
   "source": [
    "N_ANCHORS = 120000\n",
    "ANCHOR_SELECTION = 'uniform'\n",
    "\n",
    "NORMALIZATIONS = ['standard', 'l2']\n",
    "\n",
    "USE_REFINEMENT = True\n",
    "REFINEMENT_EPOCHS = 40\n",
    "REFINEMENT_LR = 5e-5\n",
    "REFINEMENT_BATCH_SIZE = 256\n",
    "REFINEMENT_PATIENCE = 8\n",
    "\n",
    "PROCRUSTES_METHOD = 'lortho'\n",
    "USE_ENSEMBLE = False\n",
    "\n",
    "print(\"\\n1. Loading training data\")\n",
    "train_data = load_data(TRAIN_DATA)\n",
    "\n",
    "text_embeddings, image_embeddings, label_matrix = prepare_train_data(train_data)\n",
    "\n",
    "text_embeddings = torch.from_numpy(train_data['captions/embeddings']).float()\n",
    "image_embeddings = torch.from_numpy(train_data['images/embeddings']).float()\n",
    "label_matrix = torch.from_numpy(train_data['captions/label']).bool()\n",
    "gt_indices = torch.argmax(label_matrix.long(), dim=1)\n",
    "\n",
    "\n",
    "n_captions = len(text_embeddings)\n",
    "n_val = min(5000, n_captions // 10)\n",
    "\n",
    "caption_indices = torch.randperm(n_captions)\n",
    "val_caption_indices = caption_indices[:n_val]\n",
    "train_caption_indices = caption_indices[n_val:]\n",
    "\n",
    "print(f\"   Training captions: {len(train_caption_indices)}\")\n",
    "print(f\"   Validation captions: {len(val_caption_indices)}\")\n",
    "\n",
    "val_data = {\n",
    "    'captions/embeddings': train_data['captions/embeddings'][val_caption_indices.numpy()],\n",
    "    'images/embeddings': train_data['images/embeddings'],\n",
    "    'captions/label': train_data['captions/label'][val_caption_indices.numpy()]\n",
    "}\n",
    "\n",
    "train_data_split = {\n",
    "    'captions/embeddings': train_data['captions/embeddings'][train_caption_indices.numpy()],\n",
    "    'images/embeddings': train_data['images/embeddings'],\n",
    "    'captions/label': train_data['captions/label'][train_caption_indices.numpy()]\n",
    "}\n",
    "\n",
    "np.savez_compressed(\n",
    "    'data/train/validation_split.npz',\n",
    "    captions_embeddings=val_data['captions/embeddings'],\n",
    "    images_embeddings=val_data['images/embeddings'],\n",
    "    captions_label=val_data['captions/label'],\n",
    "    val_caption_indices=val_caption_indices.numpy(),\n",
    "    train_caption_indices=train_caption_indices.numpy()\n",
    ")\n",
    "\n",
    "print(f\"   ✓ Saved validation split to data/splits/validation_split.npz\")\n",
    "\n",
    "print(f\"\\n2. Selecting {N_ANCHORS} anchor pairs ({ANCHOR_SELECTION})...\")\n",
    "caption_anchor_idx, image_anchor_idx = select_anchors_diverse(train_data_split, N_ANCHORS, method=ANCHOR_SELECTION)\n",
    "\n",
    "X_anchors_full, Y_anchors_full, _ = prepare_train_data(train_data_split)\n",
    "\n",
    "X_anchors = X_anchors_full[caption_anchor_idx]\n",
    "Y_anchors = Y_anchors_full[caption_anchor_idx]\n",
    "\n",
    "print(f\"   Anchor text embeddings: {X_anchors.shape}\")\n",
    "print(f\"   Anchor image embeddings: {Y_anchors.shape}\")\n",
    "\n",
    "print(f\"\\n3. Training Procrustes models\")\n",
    "print(f\"   Method: {PROCRUSTES_METHOD}\")\n",
    "\n",
    "translators = {}\n",
    "results = {}\n",
    "best_translator = None\n",
    "best_mrr = 0\n",
    "best_config = None\n",
    "\n",
    "norm = 'l2'\n",
    "\n",
    "print(f\"\\nTesting normalization: {norm}\")\n",
    "\n",
    "translator = ProcrustesTranslator(\n",
    "    normalization=norm,\n",
    "    method=PROCRUSTES_METHOD\n",
    ")\n",
    "translator.fit(X_anchors, Y_anchors)\n",
    "\n",
    "val_text = text_embeddings[val_caption_indices]\n",
    "val_gt = gt_indices[val_caption_indices]\n",
    "val_image = image_embeddings\n",
    "\n",
    "translated = translator.transform(val_text)\n",
    "\n",
    "recall_1, mrr, l2_dist = evaluate_retrieval(\n",
    "    None,\n",
    "    translated,\n",
    "    val_image,\n",
    "    val_gt,\n",
    "    DEVICE,\n",
    "    k=1\n",
    ")\n",
    "\n",
    "recall_5, _, _ = evaluate_retrieval(\n",
    "    None,\n",
    "    translated,\n",
    "    val_image,\n",
    "    val_gt,\n",
    "    DEVICE,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "recall_10, _, _ = evaluate_retrieval(\n",
    "    None,\n",
    "    translated,\n",
    "    val_image,\n",
    "    val_gt,\n",
    "    DEVICE,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "print(f\"     Validation Results:\")\n",
    "print(f\"       Recall@1:  {recall_1:.4f}\")\n",
    "print(f\"       Recall@5:  {recall_5:.4f}\")\n",
    "print(f\"       Recall@10: {recall_10:.4f}\")\n",
    "print(f\"       MRR:       {mrr:.4f}\")\n",
    "print(f\"       L2 Distance: {l2_dist:.4f}\")\n",
    "\n",
    "translators[norm] = translator\n",
    "results[norm] = {\n",
    "    'recall_1': recall_1,\n",
    "    'recall_5': recall_5,\n",
    "    'recall_10': recall_10,\n",
    "    'mrr': mrr,\n",
    "    'l2_dist': l2_dist\n",
    "}\n",
    "\n",
    "if mrr > best_mrr:\n",
    "    best_mrr = mrr\n",
    "    best_translator = translator\n",
    "    best_config = norm\n",
    "\n",
    "print(f\"\\n   → Best Procrustes: {best_config} with MRR = {best_mrr:.4f}\")\n",
    "\n",
    "print(f\"Procrustes normalization method: {best_translator.normalization}\")\n",
    "print(f\"Procrustes method: {best_translator.method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6b435d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history, save_path='training_curves.png'):\n",
    "    has_val = history.get('val_loss') is not None\n",
    "    has_components = any(k.startswith('train_') and k != 'train_loss' for k in history.keys())\n",
    "    \n",
    "    n_plots = 2 if has_components else 1\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(6*n_plots, 5))\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    ax = axes[0]\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    ax.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    if has_val:\n",
    "        ax.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.set_title('Training Progress', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    if has_components:\n",
    "        ax = axes[1]\n",
    "        \n",
    "        for key in ['train_clip', 'train_mse', 'train_cosine', 'train_contrastive', \n",
    "                    'train_similarity', 'train_group_consistency']:\n",
    "            if key in history and history[key]:\n",
    "                label = key.replace('train_', '').title()\n",
    "                ax.plot(epochs, history[key], label=label, linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "        ax.set_ylabel('Component Loss', fontsize=12)\n",
    "        ax.set_title('Loss Components', fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcec423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Neural refinement of best Procrustes model...\n",
      "   Initial validation MRR: 0.1036\n",
      "\n",
      "============================================================\n",
      "NEURAL REFINEMENT TRAINING (SigLipLoss)\n",
      "============================================================\n",
      "Training data: 120000 captions, 25000 images\n",
      "Validation data: 5000 captions\n",
      "  Initialized with Procrustes solution (extracted 1536×1024 from (1536, 1536))\n",
      "Model parameters: 8,399,361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: 100%|██████████| 469/469 [00:10<00:00, 44.34it/s, loss=118.4070] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Train Loss: 413.7385\n",
      "  Val Loss: 813.1583\n",
      "  LR: 0.000050\n",
      "  ✓ New best model! (Loss: 813.1583)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40: 100%|██████████| 469/469 [00:07<00:00, 61.58it/s, loss=75.1144] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "  Train Loss: 94.4805\n",
      "  Val Loss: 667.2427\n",
      "  LR: 0.000050\n",
      "  ✓ New best model! (Loss: 667.2427)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40: 100%|██████████| 469/469 [00:07<00:00, 58.70it/s, loss=56.1781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "  Train Loss: 71.9515\n",
      "  Val Loss: 621.2647\n",
      "  LR: 0.000050\n",
      "  ✓ New best model! (Loss: 621.2647)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40: 100%|██████████| 469/469 [00:07<00:00, 60.38it/s, loss=39.6368] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "  Train Loss: 59.8015\n",
      "  Val Loss: 439.6286\n",
      "  LR: 0.000050\n",
      "  ✓ New best model! (Loss: 439.6286)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40: 100%|██████████| 469/469 [00:07<00:00, 61.00it/s, loss=49.9999] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "  Train Loss: 51.3397\n",
      "  Val Loss: 390.6216\n",
      "  LR: 0.000050\n",
      "  ✓ New best model! (Loss: 390.6216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40: 100%|██████████| 469/469 [00:07<00:00, 60.10it/s, loss=36.0675] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "  Train Loss: 44.9923\n",
      "  Val Loss: 456.5824\n",
      "  LR: 0.000050\n",
      "  No improvement (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40: 100%|██████████| 469/469 [00:07<00:00, 61.05it/s, loss=40.6797] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "  Train Loss: 40.2761\n",
      "  Val Loss: 372.0241\n",
      "  LR: 0.000049\n",
      "  ✓ New best model! (Loss: 372.0241)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40: 100%|██████████| 469/469 [00:07<00:00, 62.51it/s, loss=30.2124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "  Train Loss: 36.3128\n",
      "  Val Loss: 423.8181\n",
      "  LR: 0.000049\n",
      "  No improvement (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40: 100%|██████████| 469/469 [00:07<00:00, 59.81it/s, loss=28.4344] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "  Train Loss: 32.7927\n",
      "  Val Loss: 316.8164\n",
      "  LR: 0.000048\n",
      "  ✓ New best model! (Loss: 316.8164)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40: 100%|██████████| 469/469 [00:07<00:00, 60.10it/s, loss=22.2474] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Summary:\n",
      "  Train Loss: 29.6091\n",
      "  Val Loss: 253.2075\n",
      "  LR: 0.000047\n",
      "  ✓ New best model! (Loss: 253.2075)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40: 100%|██████████| 469/469 [00:07<00:00, 59.31it/s, loss=20.9568] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 Summary:\n",
      "  Train Loss: 27.2778\n",
      "  Val Loss: 252.5251\n",
      "  LR: 0.000046\n",
      "  ✓ New best model! (Loss: 252.5251)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40: 100%|██████████| 469/469 [00:08<00:00, 55.47it/s, loss=26.2147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 Summary:\n",
      "  Train Loss: 25.0491\n",
      "  Val Loss: 297.9079\n",
      "  LR: 0.000045\n",
      "  No improvement (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40: 100%|██████████| 469/469 [00:08<00:00, 56.90it/s, loss=18.8137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 Summary:\n",
      "  Train Loss: 23.0415\n",
      "  Val Loss: 275.7688\n",
      "  LR: 0.000043\n",
      "  No improvement (2/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40: 100%|██████████| 469/469 [00:08<00:00, 53.59it/s, loss=17.8078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 Summary:\n",
      "  Train Loss: 20.9818\n",
      "  Val Loss: 216.5830\n",
      "  LR: 0.000042\n",
      "  ✓ New best model! (Loss: 216.5830)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40: 100%|██████████| 469/469 [00:08<00:00, 55.54it/s, loss=16.1023] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 Summary:\n",
      "  Train Loss: 19.5392\n",
      "  Val Loss: 247.8447\n",
      "  LR: 0.000040\n",
      "  No improvement (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40: 100%|██████████| 469/469 [00:08<00:00, 58.03it/s, loss=15.4613] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 Summary:\n",
      "  Train Loss: 18.0184\n",
      "  Val Loss: 241.5046\n",
      "  LR: 0.000039\n",
      "  No improvement (2/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40: 100%|██████████| 469/469 [00:08<00:00, 55.79it/s, loss=16.3782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 Summary:\n",
      "  Train Loss: 16.7204\n",
      "  Val Loss: 196.0521\n",
      "  LR: 0.000037\n",
      "  ✓ New best model! (Loss: 196.0521)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40: 100%|██████████| 469/469 [00:08<00:00, 56.21it/s, loss=16.6711] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 Summary:\n",
      "  Train Loss: 15.3249\n",
      "  Val Loss: 191.5651\n",
      "  LR: 0.000035\n",
      "  ✓ New best model! (Loss: 191.5651)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40: 100%|██████████| 469/469 [00:08<00:00, 57.88it/s, loss=12.7421] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 Summary:\n",
      "  Train Loss: 14.3898\n",
      "  Val Loss: 179.6608\n",
      "  LR: 0.000033\n",
      "  ✓ New best model! (Loss: 179.6608)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40: 100%|██████████| 469/469 [00:08<00:00, 53.65it/s, loss=12.8027] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 Summary:\n",
      "  Train Loss: 13.4520\n",
      "  Val Loss: 170.9258\n",
      "  LR: 0.000031\n",
      "  ✓ New best model! (Loss: 170.9258)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40: 100%|██████████| 469/469 [00:08<00:00, 52.48it/s, loss=11.0181] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 Summary:\n",
      "  Train Loss: 12.5144\n",
      "  Val Loss: 188.3420\n",
      "  LR: 0.000029\n",
      "  No improvement (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40: 100%|██████████| 469/469 [00:07<00:00, 58.69it/s, loss=9.7003]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 Summary:\n",
      "  Train Loss: 11.5666\n",
      "  Val Loss: 208.8699\n",
      "  LR: 0.000028\n",
      "  No improvement (2/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40: 100%|██████████| 469/469 [00:08<00:00, 54.27it/s, loss=11.3013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 Summary:\n",
      "  Train Loss: 10.6282\n",
      "  Val Loss: 142.0755\n",
      "  LR: 0.000026\n",
      "  ✓ New best model! (Loss: 142.0755)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40: 100%|██████████| 469/469 [00:08<00:00, 53.61it/s, loss=9.8441]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 Summary:\n",
      "  Train Loss: 9.9709\n",
      "  Val Loss: 158.5658\n",
      "  LR: 0.000024\n",
      "  No improvement (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40: 100%|██████████| 469/469 [00:09<00:00, 51.86it/s, loss=7.4736] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 Summary:\n",
      "  Train Loss: 9.3681\n",
      "  Val Loss: 138.3880\n",
      "  LR: 0.000022\n",
      "  ✓ New best model! (Loss: 138.3880)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40: 100%|██████████| 469/469 [00:08<00:00, 56.62it/s, loss=8.4007]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 Summary:\n",
      "  Train Loss: 8.7406\n",
      "  Val Loss: 152.0197\n",
      "  LR: 0.000020\n",
      "  No improvement (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40: 100%|██████████| 469/469 [00:07<00:00, 62.07it/s, loss=7.7045]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 Summary:\n",
      "  Train Loss: 8.1955\n",
      "  Val Loss: 145.7334\n",
      "  LR: 0.000018\n",
      "  No improvement (2/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40: 100%|██████████| 469/469 [00:08<00:00, 55.18it/s, loss=5.1429] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 Summary:\n",
      "  Train Loss: 7.7516\n",
      "  Val Loss: 125.5144\n",
      "  LR: 0.000016\n",
      "  ✓ New best model! (Loss: 125.5144)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40: 100%|██████████| 469/469 [00:08<00:00, 55.71it/s, loss=5.5451] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 Summary:\n",
      "  Train Loss: 7.2293\n",
      "  Val Loss: 123.8620\n",
      "  LR: 0.000015\n",
      "  ✓ New best model! (Loss: 123.8620)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40: 100%|██████████| 469/469 [00:08<00:00, 57.48it/s, loss=5.9224] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 Summary:\n",
      "  Train Loss: 6.8397\n",
      "  Val Loss: 130.9074\n",
      "  LR: 0.000013\n",
      "  No improvement (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40: 100%|██████████| 469/469 [00:08<00:00, 53.16it/s, loss=4.6979] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 Summary:\n",
      "  Train Loss: 6.4856\n",
      "  Val Loss: 123.6391\n",
      "  LR: 0.000012\n",
      "  ✓ New best model! (Loss: 123.6391)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40: 100%|██████████| 469/469 [00:08<00:00, 52.88it/s, loss=6.2285] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 Summary:\n",
      "  Train Loss: 6.1216\n",
      "  Val Loss: 111.4521\n",
      "  LR: 0.000010\n",
      "  ✓ New best model! (Loss: 111.4521)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40: 100%|██████████| 469/469 [00:09<00:00, 48.94it/s, loss=5.0770]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 Summary:\n",
      "  Train Loss: 5.8964\n",
      "  Val Loss: 110.4191\n",
      "  LR: 0.000009\n",
      "  ✓ New best model! (Loss: 110.4191)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40: 100%|██████████| 469/469 [00:09<00:00, 48.48it/s, loss=4.9640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 Summary:\n",
      "  Train Loss: 5.5723\n",
      "  Val Loss: 116.8246\n",
      "  LR: 0.000008\n",
      "  No improvement (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40: 100%|██████████| 469/469 [00:09<00:00, 48.37it/s, loss=4.2857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 Summary:\n",
      "  Train Loss: 5.3662\n",
      "  Val Loss: 109.5287\n",
      "  LR: 0.000007\n",
      "  ✓ New best model! (Loss: 109.5287)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40: 100%|██████████| 469/469 [00:09<00:00, 50.89it/s, loss=3.7749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36 Summary:\n",
      "  Train Loss: 5.1741\n",
      "  Val Loss: 108.4450\n",
      "  LR: 0.000006\n",
      "  ✓ New best model! (Loss: 108.4450)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40: 100%|██████████| 469/469 [00:08<00:00, 54.79it/s, loss=3.3972] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 Summary:\n",
      "  Train Loss: 4.9984\n",
      "  Val Loss: 105.1929\n",
      "  LR: 0.000006\n",
      "  ✓ New best model! (Loss: 105.1929)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40: 100%|██████████| 469/469 [00:08<00:00, 53.04it/s, loss=5.7518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38 Summary:\n",
      "  Train Loss: 4.9203\n",
      "  Val Loss: 107.5419\n",
      "  LR: 0.000005\n",
      "  No improvement (1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40: 100%|██████████| 469/469 [00:08<00:00, 55.46it/s, loss=3.2061] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 Summary:\n",
      "  Train Loss: 4.8475\n",
      "  Val Loss: 104.9796\n",
      "  LR: 0.000005\n",
      "  ✓ New best model! (Loss: 104.9796)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40: 100%|██████████| 469/469 [00:07<00:00, 62.05it/s, loss=2.9647] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40 Summary:\n",
      "  Train Loss: 4.7352\n",
      "  Val Loss: 102.4799\n",
      "  LR: 0.000005\n",
      "  ✓ New best model! (Loss: 102.4799)\n",
      "\n",
      "============================================================\n",
      "Training Complete! Best Loss: 102.4799\n",
      "============================================================\n",
      "   ✓ Training curves saved to training_curves.png\n",
      "\n",
      "   Evaluating refined model on validation set...\n",
      "   Refined Model Validation Results:\n",
      "     Recall@1:  0.1378\n",
      "     Recall@5:  0.3528\n",
      "     Recall@10: 0.4750\n",
      "     MRR:       0.1378\n",
      "     L2 Distance: 31.9781\n",
      "   Improvement: +0.0342\n",
      "   ✓ Refinement improved performance!\n",
      "After neural refinement norms: min=13.2261, max=23.4800, mean=18.6310\n",
      "Target (DINOv2) norms: min=22.3201, max=35.4587, mean=25.9392\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5000) must match the size of tensor b (125000) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 135\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget (DINOv2) norms: min=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt_images\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt_images\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt_images\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Cosine similarity comparison\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m procrustes_cos_sim \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocrustes_pred_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m refined_cos_sim \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcosine_similarity(refined_pred, gt_images, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Cosine Similarity to GT ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5000) must match the size of tensor b (125000) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "final_model = None\n",
    "\n",
    "if USE_REFINEMENT:\n",
    "    print(f\"\\n4. Neural refinement of best Procrustes model\")\n",
    "    print(f\"   Initial validation MRR: {best_mrr:.4f}\")\n",
    "\n",
    "    refined_model, history = train_refined_model_improved(\n",
    "        procrustes_translator=best_translator,\n",
    "        train_data=train_data_split,\n",
    "        val_data=val_data,\n",
    "        epochs=REFINEMENT_EPOCHS,\n",
    "        lr=REFINEMENT_LR,\n",
    "        batch_size=REFINEMENT_BATCH_SIZE,\n",
    "        hidden_dim=2048,\n",
    "        patience=REFINEMENT_PATIENCE,\n",
    "        loss_weights=None,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        plot_training_history(history, save_path='training_curves.png')\n",
    "        print(\"Training curves saved to training_curves.png\")\n",
    "    except Exception as e:\n",
    "        print(f\" Could not plot training curves: {e}\")\n",
    "\n",
    "    print(\"\\n   Evaluating refined model on validation set\")\n",
    "    \n",
    "    recall_1, mrr_refined, l2_dist = evaluate_retrieval(\n",
    "        refined_model,\n",
    "        val_text,\n",
    "        image_embeddings,\n",
    "        val_gt,\n",
    "        DEVICE,\n",
    "        1\n",
    "    )\n",
    "    \n",
    "    recall_5, _, _ = evaluate_retrieval(\n",
    "        refined_model,\n",
    "        val_text,\n",
    "        image_embeddings,\n",
    "        val_gt,\n",
    "        DEVICE,\n",
    "        5\n",
    "    )\n",
    "    \n",
    "    recall_10, _, _ = evaluate_retrieval(\n",
    "        refined_model,\n",
    "        val_text,\n",
    "        image_embeddings,\n",
    "        val_gt,\n",
    "        DEVICE,\n",
    "        10\n",
    "    )\n",
    "    \n",
    "    print(f\"   Refined Model Validation Results:\")\n",
    "    print(f\"     Recall@1:  {recall_1:.4f}\")\n",
    "    print(f\"     Recall@5:  {recall_5:.4f}\")\n",
    "    print(f\"     Recall@10: {recall_10:.4f}\")\n",
    "    print(f\"     MRR:       {mrr_refined:.4f}\")\n",
    "    print(f\"     L2 Distance: {l2_dist:.4f}\")\n",
    "    print(f\"   Improvement: {mrr_refined - best_mrr:+.4f}\")\n",
    "    \n",
    "    if mrr_refined > best_mrr:\n",
    "        print(\"Refinement improved performance!\")\n",
    "        final_model = refined_model\n",
    "        best_mrr = mrr_refined\n",
    "    else:\n",
    "        print(\"Refinement did not improve, using Procrustes only\")\n",
    "        final_model = best_translator\n",
    "else:\n",
    "    final_model = best_translator\n",
    "\n",
    "print(f\"\\n6. Final evaluation on full training set\")\n",
    "    \n",
    "if isinstance(final_model, nn.Module):\n",
    "    final_model.eval()\n",
    "    with torch.no_grad():\n",
    "        full_translated = final_model(text_embeddings.to(DEVICE)).cpu()\n",
    "else:\n",
    "    full_translated = final_model.transform(text_embeddings)\n",
    "    if not torch.is_tensor(full_translated):\n",
    "        full_translated = torch.from_numpy(full_translated).float()\n",
    "\n",
    "recall_1, mrr_train, _ = evaluate_retrieval(\n",
    "    None,\n",
    "    full_translated,\n",
    "    image_embeddings,\n",
    "    gt_indices,\n",
    "    DEVICE,\n",
    "    k=1\n",
    ")\n",
    "\n",
    "recall_5, _, _ = evaluate_retrieval(\n",
    "    None,\n",
    "    full_translated,\n",
    "    image_embeddings,\n",
    "    gt_indices,\n",
    "    DEVICE,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "recall_10, _, _ = evaluate_retrieval(\n",
    "    None,\n",
    "    full_translated,\n",
    "    image_embeddings,\n",
    "    gt_indices,\n",
    "    DEVICE,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "print(f\"   Full Training Set Results:\")\n",
    "print(f\"     Recall@1:  {recall_1:.4f}\")\n",
    "print(f\"     Recall@5:  {recall_5:.4f}\")\n",
    "print(f\"     Recall@10: {recall_10:.4f}\")\n",
    "print(f\"     MRR:       {mrr_train:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7dcd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n7. Generating submission for test set\")\n",
    "test_data = load_data(TEST_DATA)\n",
    "test_text_embeddings = torch.from_numpy(test_data['captions/embeddings']).float()\n",
    "\n",
    "if isinstance(final_model, nn.Module):\n",
    "    final_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_translated = final_model(test_text_embeddings.to(DEVICE))\n",
    "        test_translated = test_translated.cpu()\n",
    "elif hasattr(final_model, 'transform'):\n",
    "    test_translated = final_model.transform(test_text_embeddings)\n",
    "    if not torch.is_tensor(test_translated):\n",
    "        test_translated = torch.from_numpy(test_translated).float()\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model type: {type(final_model)}\")\n",
    "\n",
    "print(f\"   Test translated embeddings: {test_translated.shape}\")\n",
    "\n",
    "sample_ids = test_data['captions/ids']\n",
    "submission_file = f\"submission_{best_config.replace(' ', '_').lower()}.csv\"\n",
    "submission = generate_submission(\n",
    "    sample_ids,\n",
    "    test_translated.cpu().numpy(),\n",
    "    output_file=submission_file\n",
    ")\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint = {\n",
    "    'config': {\n",
    "        'n_anchors': N_ANCHORS,\n",
    "        'anchor_selection': ANCHOR_SELECTION,\n",
    "        'normalization': best_config,\n",
    "        'procrustes_method': PROCRUSTES_METHOD,\n",
    "        'use_refinement': USE_REFINEMENT,\n",
    "        'use_ensemble': USE_ENSEMBLE,\n",
    "    },\n",
    "    'results': {\n",
    "        'validation_mrr': best_mrr,\n",
    "        'training_mrr': mrr_train,\n",
    "        'all_results': results\n",
    "    }\n",
    "}\n",
    "\n",
    "if isinstance(final_model, nn.Module):\n",
    "    checkpoint['model_state_dict'] = final_model.state_dict()\n",
    "    torch.save(checkpoint, 'final_model_checkpoint.pt')\n",
    "    print(f\"Model checkpoint saved to final_model_checkpoint.pt\")\n",
    "\n",
    "print(\"COMPLETE!\")\n",
    "print(f\"Configuration: {best_config}\")\n",
    "print(f\"Validation MRR: {best_mrr:.4f}\")\n",
    "print(f\"Training MRR: {mrr_train:.4f}\")\n",
    "print(f\"Method: {'Refined ' if USE_REFINEMENT and isinstance(final_model, nn.Module) else ''}\"\n",
    "        f\"{'Ensemble ' if hasattr(final_model, 'translators') else ''}\"\n",
    "        f\"Procrustes ({PROCRUSTES_METHOD})\")\n",
    "print(f\"Anchors: {N_ANCHORS} ({ANCHOR_SELECTION})\")\n",
    "print(f\"Submission: {submission_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlcv",
   "language": "python",
   "name": "amlcv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
